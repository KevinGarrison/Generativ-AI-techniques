{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Embedding Techniques for Textual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "**The History and Future of Artificial Intelligence**  \n",
    "\n",
    "### 1. Introduction  \n",
    "Artificial Intelligence (AI) has transformed from a theoretical concept into a fundamental aspect of modern technology. Once confined to academic discussions, AI now powers everyday applications, from voice assistants to medical diagnostics. But how did AI evolve, and where is it headed?  \n",
    "\n",
    "### 2. The Early Days of AI  \n",
    "The idea of artificial intelligence dates back to ancient myths, where mechanical beings were depicted as sentient. However, modern AI research formally began in the 1950s. Alan Turing, a pioneer in computing, introduced the concept of machine intelligence through his famous \"Turing Test.\" In 1956, the Dartmouth Conference officially marked the birth of AI as a field. Early AI models relied on rule-based systems, which worked well for structured tasks but struggled with uncertainty and complexity.  \n",
    "\n",
    "### 3. The Rise of Machine Learning  \n",
    "By the 1990s, a shift occurredâ€”machine learning (ML) began to outperform traditional rule-based AI. Instead of manually encoding rules, ML models learned patterns from data. Breakthroughs in deep learning, powered by neural networks, revolutionized fields like image recognition, natural language processing (NLP), and game-playing AI. Notable achievements include IBMâ€™s Deep Blue defeating world chess champion Garry Kasparov in 1997 and Google DeepMindâ€™s AlphaGo outplaying human Go masters.  \n",
    "\n",
    "### 4. Modern AI Applications  \n",
    "AI now permeates various industries:  \n",
    "- **Healthcare**: AI assists in diagnosing diseases, predicting patient outcomes, and personalizing treatments.  \n",
    "- **Finance**: Fraud detection and algorithmic trading rely heavily on AI-driven analytics.  \n",
    "- **Transportation**: Autonomous vehicles use AI to navigate complex environments.  \n",
    "- **Entertainment**: Recommendation systems, such as those used by Netflix and Spotify, tailor content based on user preferences.  \n",
    "\n",
    "### 5. Challenges and Ethical Considerations  \n",
    "Despite its potential, AI faces several challenges:  \n",
    "- **Bias and Fairness**: AI models can inherit biases from training data, leading to unfair outcomes.  \n",
    "- **Privacy Concerns**: The collection of massive datasets raises questions about user privacy and data security.  \n",
    "- **Job Displacement**: Automation threatens traditional job roles, prompting discussions on reskilling the workforce.  \n",
    "- **AI Safety**: As AI systems become more autonomous, ensuring they align with human values is critical.  \n",
    "\n",
    "### 6. The Future of AI  \n",
    "Looking ahead, AI research focuses on:  \n",
    "- **Explainable AI (XAI)**: Making AI decisions transparent and interpretable.  \n",
    "- **General AI**: Developing systems that can reason and learn across multiple domains.  \n",
    "- **AI and Creativity**: From generating art to composing music, AI is expanding its creative potential.  \n",
    "- **Human-AI Collaboration**: Future AI systems will likely augment human intelligence rather than replace it.  \n",
    "\n",
    "### 7. Conclusion  \n",
    "Artificial intelligence is not just a technological evolutionâ€”it is a societal transformation. While challenges remain, AIâ€™s potential to enhance lives is undeniable. As research progresses, the key will be balancing innovation with ethical responsibility, ensuring AI serves humanity's best interests.  \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking (see chunking.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def recursive_chunking(text, chunk_size=200, overlap=50):\n",
    "    \"\"\"\n",
    "    Splits text into recursive character chunks using LangChain's RecursiveCharacterTextSplitter.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The input text to be chunked.\n",
    "        chunk_size (int): The length of each chunk.\n",
    "        overlap (int): The number of overlapping characters between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of recursively split text chunks.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    return splitter.split_text(text)\n",
    "\n",
    "\n",
    "chunks = recursive_chunking(text, chunk_size=100, overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['**The History and Future of Artificial Intelligence**',\n",
       " '### 1. Introduction',\n",
       " 'Artificial Intelligence (AI) has transformed from a theoretical concept into a fundamental aspect',\n",
       " 'fundamental aspect of modern technology. Once confined to academic discussions, AI now powers',\n",
       " 'AI now powers everyday applications, from voice assistants to medical diagnostics. But how did AI',\n",
       " 'But how did AI evolve, and where is it headed?',\n",
       " '### 2. The Early Days of AI',\n",
       " 'The idea of artificial intelligence dates back to ancient myths, where mechanical beings were',\n",
       " 'beings were depicted as sentient. However, modern AI research formally began in the 1950s. Alan',\n",
       " 'in the 1950s. Alan Turing, a pioneer in computing, introduced the concept of machine intelligence',\n",
       " 'intelligence through his famous \"Turing Test.\" In 1956, the Dartmouth Conference officially marked',\n",
       " 'officially marked the birth of AI as a field. Early AI models relied on rule-based systems, which',\n",
       " 'systems, which worked well for structured tasks but struggled with uncertainty and complexity.',\n",
       " '### 3. The Rise of Machine Learning',\n",
       " 'By the 1990s, a shift occurredâ€”machine learning (ML) began to outperform traditional rule-based AI.',\n",
       " 'rule-based AI. Instead of manually encoding rules, ML models learned patterns from data.',\n",
       " 'patterns from data. Breakthroughs in deep learning, powered by neural networks, revolutionized',\n",
       " 'revolutionized fields like image recognition, natural language processing (NLP), and game-playing',\n",
       " 'and game-playing AI. Notable achievements include IBMâ€™s Deep Blue defeating world chess champion',\n",
       " 'chess champion Garry Kasparov in 1997 and Google DeepMindâ€™s AlphaGo outplaying human Go masters.',\n",
       " '### 4. Modern AI Applications  \\nAI now permeates various industries:',\n",
       " '- **Healthcare**: AI assists in diagnosing diseases, predicting patient outcomes, and personalizing',\n",
       " 'and personalizing treatments.',\n",
       " '- **Finance**: Fraud detection and algorithmic trading rely heavily on AI-driven analytics.',\n",
       " '- **Transportation**: Autonomous vehicles use AI to navigate complex environments.',\n",
       " '- **Entertainment**: Recommendation systems, such as those used by Netflix and Spotify, tailor',\n",
       " 'and Spotify, tailor content based on user preferences.',\n",
       " '### 5. Challenges and Ethical Considerations',\n",
       " 'Despite its potential, AI faces several challenges:',\n",
       " '- **Bias and Fairness**: AI models can inherit biases from training data, leading to unfair',\n",
       " 'leading to unfair outcomes.',\n",
       " '- **Privacy Concerns**: The collection of massive datasets raises questions about user privacy and',\n",
       " 'user privacy and data security.',\n",
       " '- **Job Displacement**: Automation threatens traditional job roles, prompting discussions on',\n",
       " 'discussions on reskilling the workforce.',\n",
       " '- **AI Safety**: As AI systems become more autonomous, ensuring they align with human values is',\n",
       " 'human values is critical.',\n",
       " '### 6. The Future of AI  \\nLooking ahead, AI research focuses on:',\n",
       " '- **Explainable AI (XAI)**: Making AI decisions transparent and interpretable.',\n",
       " '- **General AI**: Developing systems that can reason and learn across multiple domains.',\n",
       " '- **AI and Creativity**: From generating art to composing music, AI is expanding its creative',\n",
       " 'its creative potential.',\n",
       " '- **Human-AI Collaboration**: Future AI systems will likely augment human intelligence rather than',\n",
       " 'rather than replace it.',\n",
       " '### 7. Conclusion',\n",
       " 'Artificial intelligence is not just a technological evolutionâ€”it is a societal transformation.',\n",
       " 'transformation. While challenges remain, AIâ€™s potential to enhance lives is undeniable. As research',\n",
       " 'As research progresses, the key will be balancing innovation with ethical responsibility, ensuring',\n",
       " \"ensuring AI serves humanity's best interests.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin.garrison/VS_Code_Projekte/Generativ-AI-techniques/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”¹ TF-IDF (Most Used in Classic NLP & Search)\n",
    "ðŸ“Œ Why?\n",
    "\n",
    "Simple & Efficient: Does not require deep learning.\n",
    "Interpretable: Words with high importance are easy to extract.\n",
    "Best for: Keyword-based search, document classification, text ranking.\n",
    "ðŸ“Œ When to Use? âœ… Traditional search engines\n",
    "âœ… Keyword extraction\n",
    "âœ… Basic text ranking tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Embeddings: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def compute_tfidf_embeddings(text_corpus):\n",
    "    \"\"\"\n",
    "    Computes TF-IDF embeddings for a given corpus.\n",
    "    \n",
    "    Parameters:\n",
    "        text_corpus (list of str): List of text documents.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: TF-IDF feature matrix.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(text_corpus)\n",
    "    return tfidf_matrix.toarray()\n",
    "\n",
    "\n",
    "tfidf_embeddings = compute_tfidf_embeddings(chunks)\n",
    "\n",
    "\n",
    "print(\"TF-IDF Embeddings:\", tfidf_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”¹ Word2Vec (Most Used for Word-Level Semantics)\n",
    "ðŸ“Œ Why?\n",
    "\n",
    "Word-level semantic similarity (e.g., king - man + woman â‰ˆ queen).\n",
    "Good for smaller datasets that lack pretraining.\n",
    "Best for: Word-level tasks, analogy reasoning, and knowledge representation.\n",
    "ðŸ”¥ Most Used Models:\n",
    "\n",
    "Googleâ€™s Pretrained Word2Vec (word2vec-google-news-300)\n",
    "FastText (for subword-based embeddings)\n",
    "ðŸ“Œ When to Use? âœ… Word-level similarity tasks\n",
    "âœ… Linguistic feature extraction\n",
    "âœ… Small-scale text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Embeddings: [-7.0266607e-03 -2.4170219e-03 -7.9815732e-03  7.5089652e-03\n",
      "  6.1785416e-03  5.1360563e-03  8.4277727e-03 -5.6208338e-04\n",
      " -9.3858130e-03  9.1005322e-03 -4.9478901e-03  7.7800592e-03\n",
      "  5.4980856e-03 -1.1094318e-03 -7.6320781e-03 -1.5357598e-03\n",
      "  6.2994468e-03 -7.0587639e-03  1.3619760e-03 -8.0927676e-03\n",
      "  8.7600825e-03 -2.8296749e-03  9.4858957e-03 -5.7215313e-03\n",
      " -9.7505888e-03 -8.5913725e-03 -4.1396548e-03  4.6929359e-03\n",
      " -3.3762614e-04  9.2479391e-03  3.1300674e-03  3.7870589e-03\n",
      "  2.9500066e-03  8.1276819e-03 -2.4023876e-03  7.5090188e-03\n",
      " -9.5718857e-03  2.8705851e-03 -7.1649021e-04  3.2881487e-04\n",
      "  6.8896785e-03 -2.9296693e-03 -2.4368847e-03 -8.8146189e-05\n",
      " -4.2771694e-04 -3.5484161e-03  6.1927396e-03 -6.5928726e-03\n",
      "  7.9680011e-03 -8.5454900e-05  2.6496109e-03  3.1652716e-03\n",
      " -2.5351156e-04  1.6863156e-03 -3.1917440e-03  4.8248223e-03\n",
      "  2.3161176e-04 -3.2821479e-03 -8.8147074e-03 -9.9258935e-03\n",
      "  3.3968233e-04 -5.7347235e-03 -1.0923446e-03 -4.3193013e-03\n",
      " -8.7397834e-03  1.1136766e-03  5.9731733e-03 -2.1260234e-03\n",
      " -7.2581898e-03  3.2271112e-03 -4.1342244e-04 -5.5046971e-03\n",
      " -1.0370428e-03 -6.6331954e-04 -3.1598704e-03 -9.8591503e-03\n",
      "  7.6105744e-03  3.7002743e-03 -2.5750590e-03  7.3183142e-03\n",
      "  4.2712298e-04  7.2010290e-03 -1.6285029e-03  7.5282529e-03\n",
      " -8.0352373e-05 -6.0555316e-03 -4.7263275e-03  9.7165788e-03\n",
      "  6.8531628e-04  1.0226646e-03  8.5743759e-03 -6.2467959e-03\n",
      " -1.7869460e-03 -8.1699062e-03 -6.6136788e-03 -8.4886942e-03\n",
      "  4.0131877e-03  2.7061331e-03  5.6226309e-03  2.5597296e-03]\n"
     ]
    }
   ],
   "source": [
    "def compute_word2vec_embeddings(text_corpus, vector_size=100, window=5, min_count=1):\n",
    "    \"\"\"\n",
    "    Computes Word2Vec embeddings for a given corpus.\n",
    "    \n",
    "    Parameters:\n",
    "        text_corpus (list of str): List of sentences.\n",
    "        vector_size (int): Dimensionality of the word vectors.\n",
    "        window (int): Maximum distance between current and predicted word.\n",
    "        min_count (int): Ignores words with total frequency lower than this.\n",
    "    \n",
    "    Returns:\n",
    "        gensim Word2Vec model.\n",
    "    \"\"\"\n",
    "    tokenized_sentences = [sentence.split() for sentence in text_corpus]\n",
    "    model = Word2Vec(sentences=tokenized_sentences, vector_size=vector_size, window=window, min_count=min_count)\n",
    "    return model\n",
    "\n",
    "word2vec_model = compute_word2vec_embeddings(chunks)\n",
    "\n",
    "print(\"Word2Vec Embeddings:\", word2vec_model.wv['Artificial'] if 'Artificial' in word2vec_model.wv else \"Word not in vocabulary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT-Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”¹ BERT-Based Embeddings (Most Used in NLP & AI Applications)\n",
    "ðŸ“Œ Why?\n",
    "\n",
    "Contextualized: Unlike TF-IDF and Word2Vec, BERT-based embeddings consider word meaning based on context.\n",
    "Pretrained Models: Hugging Face's Sentence Transformers (all-MiniLM, mpnet, distilbert) are widely used.\n",
    "Best for: Semantic search, text similarity, retrieval-augmented generation (RAG), and deep NLP tasks.\n",
    "ðŸ”¥ Most Used Models:\n",
    "\n",
    "Sentence-BERT (SBERT): sentence-transformers/all-MiniLM-L6-v2\n",
    "OpenAIâ€™s Embeddings: text-embedding-ada-002 (for LLM applications)\n",
    "ðŸ“Œ When to Use? âœ… Semantic search (e.g., FAISS, Pinecone)\n",
    "âœ… Information retrieval (RAG)\n",
    "âœ… Text similarity (e.g., recommendation systems)\n",
    "âœ… Conversational AI / Chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Embeddings: [[-0.07689003  0.03038044  0.02345188 ...  0.06512355  0.06100931\n",
      "  -0.01098409]\n",
      " [-0.03123283  0.03661765  0.04304357 ...  0.08067703  0.03505589\n",
      "   0.01680759]\n",
      " [-0.04303879 -0.00884608 -0.03495566 ...  0.09574018  0.10354136\n",
      "  -0.02250312]\n",
      " ...\n",
      " [-0.05084278  0.02494333 -0.01725427 ...  0.03963003  0.00828772\n",
      "  -0.08778213]\n",
      " [-0.08628564  0.04966684 -0.04355802 ... -0.03549082  0.09674251\n",
      "  -0.07057389]\n",
      " [ 0.00073055  0.03807557  0.02349321 ...  0.07508615 -0.00896475\n",
      "  -0.07838655]]\n"
     ]
    }
   ],
   "source": [
    "def compute_bert_embeddings(text_corpus, model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Computes BERT embeddings for a given corpus using SentenceTransformers.\n",
    "    \n",
    "    Parameters:\n",
    "        text_corpus (list of str): List of sentences.\n",
    "        model_name (str): Pretrained model name from sentence-transformers.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Sentence embeddings.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(text_corpus)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "bert_embeddings = compute_bert_embeddings(chunks)\n",
    "\n",
    "print(\"BERT Embeddings:\", bert_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

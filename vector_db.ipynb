{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage with Vector Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 65 chunks in ChromaDB.\n",
      "Result 1:\n",
      "tuning methods that can be applied on the fly. The audience\n",
      "of alignment determines whether the procedure is bidirec-\n",
      "tional or unidirectional, as well as the source and format of\n",
      "the KSBs.\n",
      "The discussion in this paper does not detail choosing the\n",
      "right scope of alignment, but such a process is clearly nec-\n",
      "essary. It may be a human-centered activity that aims for a\n",
      "reflective equilibrium in which the scope has a competence,\n",
      "transience, and audience with fairly small internal conflict\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "def process_pdf_to_chroma(pdf_path, persist_directory=\"./chroma_db\", chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    Loads a PDF, performs document-based chunking, and stores chunks dynamically in ChromaDB.\n",
    "    \n",
    "    Parameters:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        persist_directory (str): Directory to store ChromaDB.\n",
    "        chunk_size (int): Size of each chunk.\n",
    "        chunk_overlap (int): Overlap between chunks to retain context.\n",
    "    \"\"\"\n",
    "    # Load the PDF\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load()\n",
    "    \n",
    "    # Combine extracted text from all pages\n",
    "    full_text = \"\\n\".join([page.page_content for page in pages])\n",
    "    \n",
    "    # Perform document-based chunking using RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "    \n",
    "    # Convert text chunks into LangChain Document objects\n",
    "    documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "    \n",
    "    # Initialize HuggingFace embeddings model\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Initialize ChromaDB vector store\n",
    "    chroma_store = Chroma.from_documents(documents, embedding_model, persist_directory=persist_directory)\n",
    "    \n",
    "    print(f\"Stored {len(documents)} chunks in ChromaDB.\")\n",
    "    return chroma_store\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = \"test.pdf\"  \n",
    "chroma_store = process_pdf_to_chroma(pdf_path)\n",
    "\n",
    "# Example query:\n",
    "query = \"Limitations of alignment?\"\n",
    "results = chroma_store.similarity_search(query, k=1)\n",
    "\n",
    "# Display search results\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i+1}:\\n{result.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 65 chunks in FAISS.\n",
      "Result 1:\n",
      "tuning methods that can be applied on the fly. The audience\n",
      "of alignment determines whether the procedure is bidirec-\n",
      "tional or unidirectional, as well as the source and format of\n",
      "the KSBs.\n",
      "The discussion in this paper does not detail choosing the\n",
      "right scope of alignment, but such a process is clearly nec-\n",
      "essary. It may be a human-centered activity that aims for a\n",
      "reflective equilibrium in which the scope has a competence,\n",
      "transience, and audience with fairly small internal conflict\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def process_pdf_to_faiss(pdf_path, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    Loads a PDF, performs document-based chunking, and stores chunks dynamically in FAISS.\n",
    "    \n",
    "    Parameters:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        chunk_size (int): Size of each chunk.\n",
    "        chunk_overlap (int): Overlap between chunks to retain context.\n",
    "    \"\"\"\n",
    "    # Load the PDF\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load()\n",
    "    \n",
    "    # Combine extracted text from all pages\n",
    "    full_text = \"\\n\".join([page.page_content for page in pages])\n",
    "    \n",
    "    # Perform document-based chunking using RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "    \n",
    "    # Initialize Sentence Transformer model\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = model.encode(chunks)\n",
    "    \n",
    "    # Initialize FAISS index\n",
    "    embedding_size = embeddings.shape[1]\n",
    "    faiss_index = faiss.IndexFlatL2(embedding_size)\n",
    "    faiss_index.add(np.array(embeddings))\n",
    "    \n",
    "    print(f\"Stored {len(chunks)} chunks in FAISS.\")\n",
    "    return faiss_index, chunks, model\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = \"test.pdf\" \n",
    "faiss_index, chunks, model = process_pdf_to_faiss(pdf_path)\n",
    "\n",
    "# Example query:\n",
    "query = \"Limitations of alignment?\"\n",
    "query_embedding = model.encode([query])\n",
    "_, indices = faiss_index.search(np.array(query_embedding), k=1)\n",
    "\n",
    "# Display search results\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"Result {i+1}:\\n{chunks[idx]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
